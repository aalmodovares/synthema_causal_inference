# # CAUSAL INFERENCE WITH REAL DATA
#
# This notebooks aims to delineate a process to evaluate causal inference models constructing a semi-synthetic dataset from real data. The process is as follows:
#
# 1. Load real data
# 2. Fit parametric models on treated patients and control patients to predict the outcome. It does not matter if the prediction is good or not, we
# just need to have a model that can construct a semi-synthetic dataset, in which we have all potential outcomes to evaluate the causal inference model.
# It is interesting that the parametric models fitted are distinct, as it is done in IHDP setting B, where the potential outcome for control group is
#  generated as a linear combination of the features, while the treated outcome is generated as an exponential combination of the features, which
#  implies that the treatment effect is heterogeneous.
# 3. Construct a semi-synthetic dataset with the fitted models. The semi-synthetic dataset will have the same distribution of the real data, but the
# outcomes are generated by the parametric models fitted in the previous step.
# 4. Fit the causal inference model on the semi-synthetic dataset and evaluate the performance of the causal inference model.
# 5. Repeat steps 2 to 4 for different parametric models and compare the performance of the causal inference models.

# Import libraries
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import numpy as np
#import predictors of sklearn
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.naive_bayes import GaussianNB
from lifelines import CoxPHFitter
#import metrics of sklearn
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
#import cross validation
from sklearn.model_selection import StratifiedKFold
#import standard scaler
from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

from preprocess_data import load_data
from ci_models import *

## Load data
#AML dataset
database, info_dict = load_data()


# Get the features and the outcome

# define the treatment
treatment = info_dict['treatments'][0] # allogeneic hsct
# remove values for autologous hsct
auto = info_dict['treatments'][1]
database = database.loc[database[auto] != 1, :]
# reomve autologous column
database = database.drop(auto, axis=1)


# define the outcome
outcome = 'TIME TO RFS' # time to relapse free survival
event = 'RFS STATUS' # relapse free survival status

# define features
features = info_dict['features']
#remove outcome from features
features.remove(outcome)
# features.remove(treatment)
# features.remove(auto)

#compute corr
corr = database.loc[:, features + [outcome]].corr()
#suprime the features that have a correlation with the outcome greater than 0.9
# features = [feature for feature in features if abs(corr.loc[feature, outcome][0]) < 0.9]
#expand the previous loop
aa =[]
for feature in features:
    print(feature)
    try:
        pos = abs(corr.loc[feature, outcome][0]) > 0.9
    except:
        pos = abs(corr.loc[feature, outcome]) > 0.9
    if not pos:
        aa.append(feature)

features = aa


if event not in features:
    features.append(event)

#impute nans as 0
database = database.fillna(0)
#impute infinite as 0
database = database.replace([np.inf, -np.inf], 0)
# scale continuous features, save the scaler
cont_features = info_dict['continuous_vars']
scaler = StandardScaler()
database.loc[:, cont_features] = scaler.fit_transform(database.loc[:, cont_features])


# define the parametric models
models = {'Linear Regression': LinearRegression(),
          'Exponential Regression': LinearRegression(),
          'Cox Regression': CoxPHFitter()
}

# fit the parametric models
# first, fit the linear regression for the control group
# get the control group
control_group = database.loc[database[treatment] == 0, :]


# fit the linear regression
models['Linear Regression'].fit(control_group.loc[:, features].values, control_group.loc[:, outcome].values)
# fit the exponential regression for the treated group
# get the treated group
treated_group = database.loc[database[treatment] == 1, :]
# fit the exponential regression
min_y = np.min(database.loc[:, outcome])
y_pos= np.log(treated_group.loc[:, outcome] - min_y + 0.001)
models['Exponential Regression'].fit(treated_group.loc[:, features], y_pos)

# generate the potential outcomes with the fitted models
# first, generate y0 for all patients
database['y0'] = models['Linear Regression'].predict(database.loc[:, features])
# second, generate y1 for all patients
database['y1'] = np.exp(models['Exponential Regression'].predict(database.loc[:, features])) + min_y

# add noise to potential outcomes, save mu0 and mu1 as the noiselees versions, y0 and y1 as the noisy versions
# first, add noise to y0
database['mu0'] = database['y0']
database['y0'] = database['y0'] + np.random.normal(0, 1, size=database.shape[0])
# second, add noise to y1
database['mu1'] = database['y1']
database['y1'] = database['y1'] + np.random.normal(0, 1, size=database.shape[0])

# add the treatment variable
database['t'] = database[treatment]

# add the factual outcome, which is y0 where the treatment is 0 and y1 where the treatment is 1
database['y'] = database['y0']
database.loc[database['t'] == 1, 'y'] = database.loc[database['t'] == 1, 'y1']

# evaluate causal inference methods with the semi-synthetic dataset
# define the causal inference methods

config= {'slearner': {'NE':3000, 'hidden_layers': (50, 25)},
         'tlearner': {'NE':3000, 'hidden_layers': (50, 25)},
         'tarnet': {'NE': 1000, 'batch_size': 1000, 'patience':40,
                    'NL_phi': 3, 'NL_y': 2, 'NN_phi': 200, 'NN_y': 100
                    }}

ci_models = {'slearner':{"Linear Regression": LinearRegression(),
            "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
            "Neural Network": MLPRegressor(hidden_layer_sizes=config['slearner']['hidden_layers'], max_iter=config['slearner']['NE'],
                                           random_state=42)},
          'tlearner':{"Linear Regression": LinearRegression(),
            "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
            "Neural Network": MLPRegressor(hidden_layer_sizes=config['tlearner']['hidden_layers'], max_iter=config['tlearner']['NE'],
                                           random_state=42)},
          'tarnet': {'TARNET': None},
}

# make several train-test splits of the dataset
Num_exps = 10
# make splits
pehe_dict = {ci_model:{} for ci_model in ci_models.keys()}
ate_dict = {ci_model:{} for ci_model in ci_models.keys()}
for i_exp in range(1, Num_exps+1):
    print(f'Experiment {i_exp}')
    # make train-test split
    train = database.sample(frac=0.7, random_state=i_exp)
    test = database.loc[~database.index.isin(train.index), :]
    # get the features and the outcome
    x_train, t_train, y_train = train.loc[:, features], train.loc[:, 't'], train.loc[:, 'y']
    x_test, t_test, y_test = test.loc[:, features], test.loc[:, 't'], test.loc[:, 'y']
    data = {'data_train': (x_train, t_train, y_train), 'data_test': (x_test, t_test, y_test)}
    true_y = {'y_train': y_train, 'y_test': y_test, 'mu0_train': train.loc[:, 'mu0'], 'mu1_train': train.loc[:, 'mu1'],
                'mu0_test': test.loc[:, 'mu0'], 'mu1_test': test.loc[:, 'mu1']}
    # fit the causal inference models
    for ci_model in ci_models.keys():
        predicted_y = predict_causal_effects(ci_models[ci_model], data,
                                             learner=ci_model, config=config)
        pehe, ate_error = evaluate_causal_inference(ci_models[ci_model], true_y, predicted_y)
        pehe_dict[ci_model][f'exp_{i_exp}'] = pehe
        ate_dict[ci_model][f'exp_{i_exp}'] = ate_error

plot_mean_pehes(Num_exps, ci_models.keys(),  pehe_dict)












